<!DOCTYPE html>
<html class=" w-mod-ix" lang="en">

<head>
  <meta http-equiv="content-type" content="text/html; charset=UTF-8">
  <style>
    .wf-force-outline-none[tabindex="-1"]:focus {
      outline: none;
    }
  </style>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="description"
    content="JourneyDB is a large-scale generated image understanding dataset that contains 4,429,295 high-resolution Midjourney images, annotated with corresponding text prompt, image caption and visual question answering.">
  <meta name="author"
    content="Keqiang Sun, Junting Pan, Yuying Ge, Hao Li, Haodong Duan, Xiaoshi Wu, Renrui Zhang, Aojun Zhou, Zipeng Qin, Yi Wang, Jifeng Dai, Yu Qiao, Hongsheng Li">

  <title>JourneyDB: A Benchmark for Generative Image Understanding</title>

  <!-- Custom styles for this template -->
  <link href="assets/offcanvas.css" rel="stylesheet">
  <link rel="icon" href="assets/jdb_teaser_small.jpg" type="image/x-ico">

</head>

<body>
  <div class="jumbotron jumbotron-fluid">
    <div class="container"></div>
    <!-- <img src="img/oo3d_teaser.gif" style="width:30%; margin-right:0px; margin-top:0px;"> -->
    <h1 class="nerf_title_v2">JourneyDB</h1>
    <h1 class="nerf_subheader_v2">JourneyDB: A Benchmark for Generative Image Understanding</h1>
    <h1 class="nerf_subheader_v2">PrePrint</h1>
    <hr>
    <p class="authors">
      <a href="https://keqiangsun.github.io" target="_blank">Keqiang Sun</a><sup>1*</sup>,
      <a href="https://junting.github.io">Junting Pan</a><sup>1*</sup>,
      <a href="https://geyuying.github.io" target="_blank">Yuying Ge</a><sup>2</sup>,
      <a href="https://cpsxhao.github.io">Hao Li</a><sup>1</sup>,
      <a href="https://kennymckormick.github.io" target="_blank">Haodong Duan</a><sup>1</sup>,
      <a href="https://github.com/tgxs002" target="_blank">Xiaoshi Wu</a><sup>1</sup>,
      <a href="https://github.com/ZrrSkywalker">Renrui Zhang</a><sup>1</sup>,
      <br>
      <a href="https://scholar.google.com/citations?user=cC8lXi8AAAAJ&hl=en" target="_blank">Aojun Zhou</a><sup>1</sup>,
      <a href="https://www.linkedin.cn/incareer/in/zipeng-bruce-qin-846a65119" target="_blank">Zipeng
        Qin</a><sup>1</sup>,
      <a href="https://shepnerd.github.io" target="_blank">Yi Wang</a><sup>4</sup>,
      <a href="https://jifengdai.org" target="_blank">Jifeng Dai</a><sup>3</sup>,
      <a href="http://mmlab.siat.ac.cn/yuqiao/" target="_blank">Yu Qiao</a><sup>4</sup>,
      <a href="https://www.ee.cuhk.edu.hk/~hsli/" target="_blank">Hongsheng Li</a><sup>1</sup>✉
    </p>

    <h1 class="nerf_subheader_v2">
      <sup>1</sup>CUHK,
      <sup>2</sup>HKU,
      <sup>3</sup>THU,
      <sup>4</sup>PJLab,
      <!-- <sup>6</sup>Centre of Perceptual and Interactive Intelligence -->
    </h1>
    <!-- <div class="nerf_equal_v2"><span class="text-span_nerf">*</span><span class="text-span_nerf_star">+</span>Corresponding Author</div> -->

    <!-- </br></br> -->
    <div class="btn-group" role="group" aria-label="Top menu">
      <a class="btn btn-primary" href="" target="_blank">Paper</a>
      <a class="btn btn-primary" href="" target="_blank">Code</a>
      <a class="btn btn-primary" href="" target="_blank">Dataset</a>
    </div>
  </div>


  <div class="container">

    <!--<hr>-->
    <br><br>
    <div data-anchor="slide1" class="section nerf_section">
      <div class="grey_container w-container">
        <h2 class="grey-heading_nerf">
          <b>Abstract</b>
        </h2>
        <p class="paragraph-3 nerf_text">
          we present a large-scale dataset, JourneyDB, for multi-modal visual understanding in the realm of generative
          images.
        </p>
        <img src="assets/jdb_teaser_small.jpg" style="width:95%; margin-right:0px; margin-top:0px;">
        <p class="paragraph-3 nerf_text">
          JourneyDB is a large-scale generated image understanding dataset that contains <b>4,429,295</b>
          high-resolution Midjourney images, annotated with corresponding <b>text prompt</b>, <b>image caption</b>, and
          <b>visual question answering</b>.
        </p>
        <p class="paragraph-3 nerf_text">
          With the vast exploration space offered by JourneyDB, we carefully set up four evaluation tracks:
          <b>a) Prompt Inversion</b>, <b>b) Style Retrieval</b>, <b>c) Image Caption</b>, and <b>d) Visual Question
            Answering</b>.
        </p>
      </div>
    </div>

    <br><br>
    <div class="section">
      <s2> Dataset Details</s2>
      <!-- <h2 class="grey-heading_nerf">
                <b>OmniObject3D Dataset</b>
            </h2> -->
      <hr>
      <!-- <p class="paragraph-3 nerf_text">
                
                OO3D is good
            </p> -->

      <h2 class="grey-heading_nerf">
        <b>Data Collection</b>
      </h2>
      <p class="paragraph-3 nerf_text">
        As illustrated in Figure 1, for each image instance, we acquire the corresponding text prompts used to generate the images with Midjourney. Furhtermore, we employ the GPT3.5 to generate the caption and VAQ groundtruth.
      </p>
      <div class="columns-5 w-row">
        <img src="assets/jdb_data_collection.jpg" style="width:95%; margin-right:0px; margin-top:10px;margin-bottom:0px;">
      </div>
      <br>
      <p class="paragraph-3 nerf_text" ,="" style="text-align:center">
        Figure 1. Data Collection Procedure.
      </p>


      <h2 class="grey-heading_nerf">
        <b>Data Instances</b>
      </h2>
      <p class="paragraph-3 nerf_text">
        We provide several examples to show the contents of each instance of the dataset in Figure 2.
      </p>
      

      <img src="assets/jdb_samples_small.jpeg" style="width:90%; margin-right:0px; margin-top:0px;">
      <br>
      <p class="paragraph-3 nerf_text" ,="" style="text-align:center">
        Figure 2. Samples of JourneyDB.
      </p>



      <h2 class="grey-heading_nerf">
        <b>Data Splits</b>
      </h2>
      <p class="paragraph-3 nerf_text">
        We provide detailed statistics for each split subset in the Table 1. We randomly split the whole dataset into roughly 20:1 to obtain the training and validation set. The training set contains 4,189,737 labeled images and 1,385,317 labeled prompts. The validation set contains 235,156 images and 82,093 prompts. And we additionally sample a testing set for manual filtering. The testing set contains 5,402 images and 5,171 prompts.
      </p>
      <div class="columns-5 w-row">
        <style type="text/css">
          .tg {
            border-collapse: collapse;
            border-color: #ccc;
            border-spacing: 0;
          }

          .tg td {
            background-color: #fff;
            border-color: #ccc;
            border-style: solid;
            border-width: 1px;
            color: #333;
            font-family: Arial, sans-serif;
            font-size: 14px;
            overflow: hidden;
            padding: 3px 3px;
            word-break: normal;
          }

          .tg th {
            background-color: #f0f0f0;
            border-color: #ccc;
            border-style: solid;
            border-width: 1px;
            color: #333;
            font-family: Arial, sans-serif;
            font-size: 14px;
            font-weight: normal;
            overflow: hidden;
            padding: 10px 5px;
            word-break: normal;
          }

          .tg .tg-baqh {
            text-align: center;
            vertical-align: top
          }

          .tg .tg-6qw1 {
            background-color: #c0c0c0;
            text-align: center;
            vertical-align: top
          }

          .tg .tg-6qw2 {
            background-color: #e4e1e1;
            text-align: center;
            vertical-align: top
          }
        </style>
        <!-- <table class="tg" style="undefined;table-layout: fixed; width: 582px; margin-left:auto;margin-right:auto;">
                    <colgroup>
                    <col style="width: 90px">
                    <col style="width: 50px">
                    <col style="width: 50px">
                    <col style="width: 50px">
                    <col style="width: 70px">
                    <col style="width: 70px">
                    <col style="width: 70px">
                    </colgroup>
                    <thead>
                      <tr>
                        <th class="tg-6qw1">Dataset</th>
                        <th class="tg-6qw1">Real</th>
                        <th class="tg-6qw1">Full 3D</th>
                        <th class="tg-6qw1">Video</th>
                        <th class="tg-6qw1">#Objects</th>
                        <th class="tg-6qw1">#Classes </th>
                        <th class="tg-6qw1"> R<sup>LVIS</sup>(%)</th>
                      </tr>
                    </thead>
                    <tbody>
                      <tr>
                        <td class="tg-baqh"><a href="https://shapenet.org/" target="_blank"> ShapeNet</a></td>
                        <td class="tg-baqh"></td>
                        <td class="tg-baqh">✓</td>
                        <td class="tg-baqh"></td>
                        <td class="tg-6qw2"><b>51k</b></td>
                        <td class="tg-baqh">55</td>
                        <td class="tg-baqh">4.1</td>
                      </tr>
                      <tr>
                        <td class="tg-baqh"><a href="https://modelnet.cs.princeton.edu/" target="_blank"> ModelNet</a></td>
                        <td class="tg-baqh"></td>
                        <td class="tg-baqh">✓</td>
                        <td class="tg-baqh"></td>
                        <td class="tg-baqh">12k</td>
                        <td class="tg-baqh">40</td>
                        <td class="tg-baqh">2.4</td>
                      </tr>
                    </tbody>
                    </table> -->
        <table class="tg" style="undefined;table-layout: fixed; width: 690px; margin-left:auto;margin-right:auto;">
          <colgroup>
            <col style="width: 90px">
            <col style="width: 70px">
            <col style="width: 70px">
            <col style="width: 95px">
            <col style="width: 95px">
            <col style="width: 70px">
            <col style="width: 70px">
          </colgroup>
          <thead>
            <tr>
              <th class="tg-6qw1"></th>
              <th class="tg-6qw1">Image</th>
              <th class="tg-6qw1">Prompt</th>
              <th class="tg-6qw1">Labeled Image</th>
              <th class="tg-6qw1">Labeled Prompt</th>
              <th class="tg-6qw1">Style QA</th>
              <th class="tg-6qw1">Content QA</th>
            </tr>
          </thead>
          <tr>
            <td>Training Set</td>
            <td>4,453,193</td>
            <td>1,643,375</td>
            <td>4,189,737</td>
            <td>1,385,317</td>
            <td>7,056,394</td>
            <td>8,775,971</td>
          </tr>
          <tr>
            <td>Validation Set</td>
            <td>234,156</td>
            <td>82,093</td>
            <td>234,156</td>
            <td>82,093</td>
            <td>311,569</td>
            <td>374,310</td>
          </tr>
          <tr>
            <td>Testing Set</td>
            <td>5,402</td>
            <td>5,171</td>
            <td>5,402</td>
            <td>5,171</td>
            <td>10,040</td>
            <td>11,369</td>
          </tr>
          <tr>
            <td>Total</td>
            <td>4,692,751</td>
            <td>1,730,639</td>
            <td>4,429,295</td>
            <td>1,472,581</td>
            <td>7,378,003</td>
            <td>9,161,650</td>
          </tr>
        </table>
      </div>
      <br>
      <p class="paragraph-3 nerf_text" ,="" style="text-align:center">
        Table 1. Data Split Statistics.
      </p>

      <br>
    </div>

    <div class="section">
      <s2>Benchmarks</s2>
      <hr>
      <h2 class="grey-heading_nerf">
        <b>Prompt Inversion</b>
      </h2>
      <p class="paragraph-3 nerf_text">
        With the proposed dataset, we set up the task: prompt inversion, which takes a single image and predicts the corresponding prompts.
        we set up a benchmark for the zero-shot prompt inversion task, where we leverage state-of-the-art multi-modal models. We adjust different prompts for each model to help them perform their best in this novel task.
        We evaluate these models on the test set of our model, and the results are presented in Table 2. In the experiment, we notice that the existing models find it hard to capture the details and the style-relevant information about the input image, and do not perform as well as in traditional datasets.
      </p>

      <style type="text/css">
        .tg  {border-collapse:collapse;border-spacing:0;margin:0 auto;}
        .tg td{border-color:inherit;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px;
          overflow:hidden;padding:10px 5px;word-break:normal;}
        .tg th{border-color:inherit;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px;
          font-weight:normal;overflow:hidden;padding:10px 5px;word-break:normal;}
        .tg .tg-9wq8{border-color:inherit;text-align:center;vertical-align:middle}
        .tg .tg-wa1i{font-weight:bold;text-align:center;vertical-align:middle}
        .tg .tg-c3ow{border-color:inherit;text-align:center;vertical-align:top}
        .tg .tg-0lax{text-align:left;vertical-align:top}
        .tg .tg-nrix{text-align:center;vertical-align:middle}
        .tg .tg-0pky{border-color:inherit;text-align:left;vertical-align:top}
        .tg .tg-7btt{border-color:inherit;font-weight:bold;text-align:center;vertical-align:top}
        .tg .tg-uzvj{border-color:inherit;font-weight:bold;text-align:center;vertical-align:middle}
        </style>
        <table class="tg">
        <thead>
          <tr>
            <th class="tg-nrix" rowspan="2">Mode</th>
            <th class="tg-nrix" rowspan="2">Models</th>
            <th class="tg-c3ow" colspan="5">Validation</th>
            <th class="tg-9wq8" colspan="7"><span style="font-weight:400;font-style:normal;text-decoration:none;color:black">Test</span></th>
          </tr>
          <tr>
            <th class="tg-c3ow">BLEU-4</th>
            <th class="tg-c3ow">METEOR</th>
            <th class="tg-c3ow">ROUGE-L</th>
            <th class="tg-c3ow">CIDEr</th>
            <th class="tg-c3ow">Similarity</th>
            <th class="tg-9wq8"><span style="font-weight:400;font-style:normal;text-decoration:none;color:black">BLEU-4</span></th>
            <th class="tg-9wq8"><span style="font-weight:400;font-style:normal;text-decoration:none;color:black">METEOR</span></th>
            <th class="tg-9wq8"><span style="font-weight:400;font-style:normal;text-decoration:none;color:black">ROUGE-L</span></th>
            <th class="tg-9wq8"><span style="font-weight:400;font-style:normal;text-decoration:none;color:black">CIDEr</span></th>
            <th class="tg-9wq8"><span style="font-weight:400;font-style:normal;text-decoration:none;color:black">Similarity</span></th>
            <th class="tg-nrix"><span style="font-weight:400;font-style:normal;text-decoration:none;color:black">QASs</span></th>
            <th class="tg-nrix"><span style="font-weight:400;font-style:normal;text-decoration:none;color:black">QASc</span></th>
          </tr>
        </thead>
        <tbody>
          <tr>
            <td class="tg-nrix" rowspan="4">ZeroShot</td>
            <td class="tg-0pky">BLIP-2 OPT</td>
            <td class="tg-c3ow">0.18</td>
            <td class="tg-c3ow">2.39</td>
            <td class="tg-c3ow">6.75</td>
            <td class="tg-c3ow">5.42</td>
            <td class="tg-c3ow">0.36</td>
            <td class="tg-9wq8"><span style="font-weight:400;font-style:normal;text-decoration:none;color:black">0.29</span></td>
            <td class="tg-9wq8"><span style="font-weight:400;font-style:normal;text-decoration:none;color:black">2.85</span></td>
            <td class="tg-9wq8"><span style="font-weight:400;font-style:normal;text-decoration:none;color:black">7.06</span></td>
            <td class="tg-9wq8"><span style="font-weight:400;font-style:normal;text-decoration:none;color:black">6.46</span></td>
            <td class="tg-9wq8"><span style="font-weight:400;font-style:normal;text-decoration:none;color:black">0.36</span></td>
            <td class="tg-nrix"><span style="font-weight:400;font-style:normal;text-decoration:none;color:black">12.42%</span></td>
            <td class="tg-nrix"><span style="font-weight:400;font-style:normal;text-decoration:none;color:black">18.55%</span></td>
          </tr>
          <tr>
            <td class="tg-0pky">BLIP-2 FlanT5</td>
            <td class="tg-c3ow">0.27</td>
            <td class="tg-c3ow">2.46</td>
            <td class="tg-c3ow">7.19</td>
            <td class="tg-c3ow">6.88</td>
            <td class="tg-c3ow">0.38</td>
            <td class="tg-9wq8"><span style="font-weight:400;font-style:normal;text-decoration:none;color:black">0.4</span></td>
            <td class="tg-9wq8"><span style="font-weight:400;font-style:normal;text-decoration:none;color:black">2.95</span></td>
            <td class="tg-9wq8"><span style="font-weight:400;font-style:normal;text-decoration:none;color:black">7.69</span></td>
            <td class="tg-9wq8"><span style="font-weight:400;font-style:normal;text-decoration:none;color:black">8.86</span></td>
            <td class="tg-9wq8"><span style="font-weight:400;font-style:normal;text-decoration:none;color:black">0.37</span></td>
            <td class="tg-nrix"><span style="font-weight:400;font-style:normal;text-decoration:none;color:black">13.79%</span></td>
            <td class="tg-nrix"><span style="font-weight:400;font-style:normal;text-decoration:none;color:black">18.58%</span></td>
          </tr>
          <tr>
            <td class="tg-0pky">MiniGPT-4</td>
            <td class="tg-c3ow">1.49</td>
            <td class="tg-c3ow">5.5</td>
            <td class="tg-c3ow">12.51</td>
            <td class="tg-c3ow">10.39</td>
            <td class="tg-c3ow">0.43</td>
            <td class="tg-9wq8"><span style="font-weight:400;font-style:normal;text-decoration:none;color:black">1.71</span></td>
            <td class="tg-9wq8"><span style="font-weight:400;font-style:normal;text-decoration:none;color:black">6.51</span></td>
            <td class="tg-9wq8"><span style="font-weight:400;font-style:normal;text-decoration:none;color:black">13.13</span></td>
            <td class="tg-9wq8"><span style="font-weight:400;font-style:normal;text-decoration:none;color:black">11.4</span></td>
            <td class="tg-9wq8"><span style="font-weight:400;font-style:normal;text-decoration:none;color:black">0.43</span></td>
            <td class="tg-nrix"><span style="font-weight:400;font-style:normal;text-decoration:none;color:black">17.12%</span></td>
            <td class="tg-wa1i"><span style="font-style:normal;text-decoration:none;color:black">26.79%</span></td>
          </tr>
          <tr>
            <td class="tg-0pky">Uni-Perceiver v2</td>
            <td class="tg-c3ow">0.23</td>
            <td class="tg-c3ow">2.44</td>
            <td class="tg-c3ow">9.11</td>
            <td class="tg-c3ow">12.38</td>
            <td class="tg-c3ow">0.33</td>
            <td class="tg-9wq8"><span style="font-weight:400;font-style:normal;text-decoration:none;color:black">0.37</span></td>
            <td class="tg-9wq8"><span style="font-weight:400;font-style:normal;text-decoration:none;color:black">2.73</span></td>
            <td class="tg-9wq8"><span style="font-weight:400;font-style:normal;text-decoration:none;color:black">9.88</span></td>
            <td class="tg-9wq8"><span style="font-weight:400;font-style:normal;text-decoration:none;color:black">15.45</span></td>
            <td class="tg-9wq8"><span style="font-weight:400;font-style:normal;text-decoration:none;color:black">0.34</span></td>
            <td class="tg-nrix"><span style="font-weight:400;font-style:normal;text-decoration:none;color:black">12.43%</span></td>
            <td class="tg-nrix"><span style="font-weight:400;font-style:normal;text-decoration:none;color:black">18.49%</span></td>
          </tr>
          <tr>
            <td class="tg-0lax">Finetune</td>
            <td class="tg-0pky">Uni-Perceiver v2</td>
            <td class="tg-7btt">20.6</td>
            <td class="tg-7btt">16.9</td>
            <td class="tg-7btt">29.1</td>
            <td class="tg-7btt">123.2</td>
            <td class="tg-7btt">0.59</td>
            <td class="tg-uzvj"><span style="font-style:normal;text-decoration:none;color:black">4.68</span></td>
            <td class="tg-uzvj"><span style="font-style:normal;text-decoration:none;color:black">8.56</span></td>
            <td class="tg-uzvj"><span style="font-style:normal;text-decoration:none;color:black">16.98</span></td>
            <td class="tg-uzvj"><span style="font-style:normal;text-decoration:none;color:black">34.01</span></td>
            <td class="tg-uzvj"><span style="font-style:normal;text-decoration:none;color:black">0.51</span></td>
            <td class="tg-wa1i"><span style="font-style:normal;text-decoration:none;color:black">19.71%</span></td>
            <td class="tg-nrix"><span style="font-weight:400;font-style:normal;text-decoration:none;color:black">24.84%</span></td>
          </tr>
        </tbody>
        </table>

        <br>

        <p class="paragraph-3 nerf_text" ,="" style="text-align:left">
          Table 2. Prompt Inversion. The existing models find it hard to capture the details and the style-relevant information about the input image, and do not perform as well as in traditional datasets.
        </p>

      <h2 class="grey-heading_nerf">
        <b>Style Retrieval</b>
      </h2>
      <p class="paragraph-3 nerf_text">
        Considering the huge style space, recognizing the style of a given image is super challenging, even for human beings. Therefore, style retrieval is desired to help people better understand the style of a given image. Retrieving a style prompt directly from a super large number of candidates is sophisticated and timeconsuming. Hence we cluster the style prompts into 344 categories, including, camera parameters, lighting, artist style, colour schemes, etc., as introduced in Section 3.2, and do the style prompt retrieval in each category, which significantly narrows the searching space. To set up the benchmark, we employ CLIP to perform a zero-shot style retrieval evaluation. The results are shown in Table 5. We notice the retrieval in the overall style prompts space results in super low recall. And the model performs much better when retrieving on the per-category sub-space.
      </p>

        
      <style type="text/css">
        .tg  {border-collapse:collapse;border-spacing:0;}
        .tg td{border-color:inherit;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px;
          overflow:hidden;padding:10px 5px;word-break:normal;}
        .tg th{border-color:inherit;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px;
          font-weight:normal;overflow:hidden;padding:10px 5px;word-break:normal;}
        .tg .tg-cly1{text-align:left;vertical-align:middle}
        .tg .tg-nrix{text-align:center;vertical-align:middle}
        </style>
        <div style="text-align: center;"></div>
        <table class="tg">
        <thead>
          <tr>
            <th class="tg-nrix" rowspan="2"><span style="font-weight:400;font-style:normal;text-decoration:none;color:black">Method</span></th>
            <th class="tg-nrix" colspan="2"><span style="font-weight:400;font-style:normal;text-decoration:none;color:black">Validation</span></th>
            <th class="tg-nrix" colspan="2"><span style="font-weight:400;font-style:normal;text-decoration:none;color:black">Test</span></th>
          </tr>
          <tr>
            <th class="tg-nrix"><span style="font-weight:400;font-style:normal;text-decoration:none;color:black">Over-All</span></th>
            <th class="tg-nrix"><span style="font-weight:400;font-style:normal;text-decoration:none;color:black">Per-Category</span></th>
            <th class="tg-nrix"><span style="font-weight:400;font-style:normal;text-decoration:none;color:black">Overall</span></th>
            <th class="tg-nrix"><span style="font-weight:400;font-style:normal;text-decoration:none;color:black">Per-Category</span></th>
          </tr>
        </thead>
        <tbody>
          <tr>
            <td class="tg-cly1"><span style="font-weight:400;font-style:normal;text-decoration:none;color:black">CLIP-ViT-L/14</span></td>
            <td class="tg-nrix"><span style="font-weight:400;font-style:normal;text-decoration:none;color:black">0.65%</span></td>
            <td class="tg-nrix"><span style="font-weight:bold;font-style:normal;text-decoration:none;color:black">41.72%</span></td>
            <td class="tg-nrix"><span style="font-weight:400;font-style:normal;text-decoration:none;color:black">47.00%</span></td>
            <td class="tg-nrix"><span style="font-weight:bold;font-style:normal;text-decoration:none;color:black">41.33%</span></td>
          </tr>
        </tbody>
        </table>

        <br>

        <p class="paragraph-3 nerf_text" ,="" style="text-align:center">
          Table 3. Style Retrieval. The model performs much better when retrieving on the per-category sub-space.
        </p>
        </div>

      <h2 class="grey-heading_nerf">
        <b>Image Caption</b>
      </h2>
      <p class="paragraph-3 nerf_text">
        Image captioning asks a multi-modal model to generate a text description for the visual content of an image. Compared to existing image captioning benchmarks (COCO Caption, etc.), JourneyDB includes both in-detail descriptions and high-level summarizations of images, assessing the model’s capability of finegrained recognition and holistic understanding.
        We evaluate existing multi-modal models on the image captioning sub-task of JourneyDB. The results are demonstrated in Figure 3, indicating that providing good descriptions for AI-generated contents is challenging for multi-modal models trained on natural images.
      </p>

      <img src="assets/jdb_caption_small.jpg" style="width:90%; margin-right:0px; margin-top:0px;">
      <br>
      <p class="paragraph-3 nerf_text" ,="" style="text-align:center">
        Figure 3. Image Caption. The examples show that existing multi-modal models failed to recognize some key concepts from the AI-generated images.
      </p>

      <h2 class="grey-heading_nerf">
        <b>Visual Question Answering</b>
      </h2>
      <p class="paragraph-3 nerf_text">
        JourneyDB consists of images with abundant and diverse prompts, which describe not only the stylistic attributes but also the visual contents of the generated images. Given the prompts of the images, we construct two tasks of multiple-choice visual question answering (MC-VQA) to evaluate the model’s ability for comprehending the style and content of generative data respectively.
        The evaluation results of the content-relevant and style-relevant zero-shot multiple-choice visual question answering are shown in Figure 4. We can observe that the performance of the existing multimodal models are far away from satisfactory on both the content-relevant and style-relevant MC-VQA.
      </p>
      
      <img src="assets/jdb_vqa_small.jpg" style="width:90%; margin-right:0px; margin-top:0px;">
      <br>
      <p class="paragraph-3 nerf_text" ,="" style="text-align:center">
        Figure 4. Visual Question Answering.
      </p>

      <!-- <p class="paragraph-3 nerf_text">
      We show examples of dense-view surface reconstruction by <a href="https://lingjie0206.github.io/papers/NeuS/">NeuS</a>. More results on the sparse-view setting can be found in the paper.
    </p> -->

    <s2>Acquirements</s2>
      <hr>
      <h2 class="grey-heading_nerf">
        <b>Downloads</b>
      </h2>
      <p class="paragraph-3 nerf_text">
        Please fill in the <a href="https://docs.google.com/forms/d/e/1FAIpQLSeiciK0g0IA46_hFaitRhdpihhpjqt3helJNT68y-C8MfKhiQ/viewform?usp=sf_link">form</a> to acquire the download link.
      </p>
      <h2 class="grey-heading_nerf">
        <b>License</b>
      </h2>
      <p class="paragraph-3 nerf_text">
        The JourneyDB dataset is available under the customized <a href="./assets/Terms_of_Usage.md">Terms of Usage</a>.
      </p>


      <!-- </br></br></br>
    <s2> Acknowledgements </s2>
    <hr>
    <p class="paragraph-3 nerf_text">
        We would like to thank  Yannick Hold-Geoffroy for his useful suggestion in video animation,
        Qiangeng Xu for providing some baseline results,
        and, Katja Schwarz and Michael Niemeyer for providing helpful video materials.
        This project was supported by NSF grant IIS-1764078 and gift money from VIVO.
    </p> -->

      <br><br>
      <!--    <div class="section">-->
      <s2>Bibtex</s2>
      <hr>
      <div class="bibtexsection">
        @article{
        }
      </div>
      <!--    </div>-->
    </div>
    <hr>

    <footer>
      <p>This website is partially borrowed from <a href="https://omniobject3d.github.io"
          target="_blank">OmniObject3D</a>.
      </p>
    </footer>
  </div>


  <script src="assets/jquery-3.5.1.slim.min.js"
    integrity="sha384-DfXdz2htPH0lsSSs5nCTpuj/zy4C+OGpamoFVy38MVBnE+IbbVYUew+OrCXaRkfj"
    crossorigin="anonymous"></script>
  <script src="assets/popper.min.js" integrity="sha384-Q6E9RHvbIyZFJoft+2mJbHaEWldlvI9IOYy5n3zV9zzTtmI3UksdQRVvoxMfooAo"
    crossorigin="anonymous"></script>
  <script src="assets/bootstrap.min.js"
    integrity="sha384-OgVRvuATP1z7JjHLkuOU7Xw704+h835Lr+6QL9UvYjZE3Ipu6Tp75j7Bh/kR0JKI"
    crossorigin="anonymous"></script>

  <script src="assets/jquery-3.5.1.min.dc5e7f18c8.js" type="text/javascript"
    integrity="sha256-9/aliU8dGd2tb6OSsuzixeV4y/faTqgFtohetphbbj0=" crossorigin="anonymous"></script>
  <script src="assets/webflow.fd6c33218.js" type="text/javascript"></script>

  <!--[if lte IE 9]><script src="//cdnjs.cloudflare.com/ajax/libs/placeholders/3.0.2/placeholders.min.js"></script><![endif]-->



</body>

</html>